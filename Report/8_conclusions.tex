The main goal of this work was to explore two ways of using kernel PCA for word embeddings: the idea of enriching word embeddings, generated by a skip-gram model, with morphological information, and of using kernel PCA to perform dimensionality reduction after fitting the embeddings without any initialization. In particular, we tested the robustness of these methods to a smaller training vocabulary size, exploring the effects of employing different dimensionalities and of hyperparameters tuning. 

% Would the same increased performance that the authors of \cite{gupta_improving_2019} observe be obtained with a smaller training vocabulary size, compared to the one originally used?
As expected, we found that the model in \cite{gupta_improving_2019} achieves decent accuracy in initial iterations, but that using a low vocabulary size strongly impacts the said accuracy. After a couple of epochs of training, our implementation sees the semantical information encoded by word2vec take precedence over the injected morphological information.
Therefore, \emph{the same increased performance cannot be obtained with a smaller training vocabulary size, compared to the one used in \cite{gupta_improving_2019}}.


% Are the models' performances sensitive to hyperparameter tuning on the Word2vec model? That is, can a similar improvement as that observed in \cite{gupta_improving_2019} be obtained, without any application of kernelization, by simply using an implementation of Word2vec which trains better and faster thanks to advances in optimization techniques and hyperparameter tuning?
We note that morphological information results in sensible embeddings which allow words with similar meanings to start close. We see this happening with ``statistical'' and ``stats'', for example. Therefore, we conclude that kernel PCA can be a valuable tool for warm-starting embedding models \emph{when a large vocabulary size is used}.
Computing kernel PCA is faster than training word2vec for an epoch, even if the vocabulary size is large, therefore the trade-off is acceptable.
However, we observe that \emph{a similar improvement as that observed in \cite{gupta_improving_2019} can be obtained, without any application of kernelization, by simply using a better implementation of Word2vec}.


% How do the results in  \cite{raunak_simple_2017} vary with respect to how aggressive the dimensionality reduction is?
Concerning ex-post dimensionality reduction, we find that kernel PCA is effective in reducing dimensionality, but this entails a cost in embedding accuracy.
For applications where memory or bandwidth play a crucial role, like in embedded systems, the reduction of the output embeddings could prove very useful.
By reducing the size to $1/8$ of the original 128-dimensional embedding with a size of 150MB to 16 dimensions the file size could be reduced to around 19MB with a decrease of $8\%$ or less in embedding quality (see \cref{table:quant-1k-post}).
This confirms the authors' conclusions in \cite{raunak_simple_2017} and could be used as a basis for an automatic reduction of embedding dimensionality when faced with memory constraints.
The results in \cite{raunak_simple_2017}, therefore, \emph{vary linearly with respect to dimensionality only when at least three epochs of training are allowed.}
\emph{With less training, moderately aggressive dimensionality reduction fares better than either very light or very heavy reduction.}


\textit{Future work} could expand on this topic from different perspectives. One could concern the usage of different kernels with lower-dimensional feature maps, which should however be chosen carefully, taking into account the kind of data analyzed and the scope of the analysis. Then, different criteria to build the training vocabulary could be explored: we would be interested in the effects of basing the choice not only on frequency but also on word length. This is also related to the choice of a string similarity measure. While the \textit{3-gram} similarity seems to be a very good option for English Wikipedia pages, other alternatives could be explored when dealing with other languages and corpora. Finally, an interesting expansion would imply applying both the techniques we reported here, performing KPCA both prior and posterior to word embedding.
