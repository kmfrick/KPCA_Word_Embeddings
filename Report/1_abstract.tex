Kernel Principal Component Analysis has proven to be useful in improving the understanding of semantic and syntactic rules from texts.
In the literature, kernelization has been applied prior to word embedding, to initialize the embedding layer with meaningful morphological information, or after it, to reduce the dimensionality of the embeddings. 
In this work we move from these two established methodologies, testing their robustness to a smaller training vocabulary size and to the usage of a different dataset.
We evaluate their performances for different choices of embedding dimensionalities, both qualitatively and quantitatively, by comparing our performance to the state of the art.
Our results show that using morphological information for initialization is less important when using more modern and advanced implementations of the word embedding model.
On the other hand, we show that kernel PCA can be applied to trained embeddings to effectively achieve dimensionality reduction.