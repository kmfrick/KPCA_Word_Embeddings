
@article{luhn_statistical_1957,
	title = {A {Statistical} {Approach} to {Mechanized} {Encoding} and {Searching} of {Literary} {Information}},
	volume = {1},
	issn = {0018-8646, 0018-8646},
	url = {http://ieeexplore.ieee.org/document/5392697/},
	doi = {10.1147/rd.14.0309},
	abstract = {Written communication of ideas is carried out on the basis of statistical probability in that a writer chooses that level of subject specificity and that combination of words which he feels will convey the most meaning. Since this process varies among individuals and since similar ideas are therefore relayed at different levels of specificity and by means of different words, the problem of literature searching by machines still presents major difficulties. A statistical approach to this problem will be outlined and the various steps of a system based on this approach will be described. Steps include the statistical analysis of a collection of documents in a field of interest, the establishment of a set of "notions" and the vocabulary by which they are expressed, the compilation of a thesaurus-type dictionary and index, the automatic encoding of documents by machine with the aid of such a dictionary, the encoding of topological notations (such as branched structures), the recording of the coded information, the establishment of a searching pattern for finding pertinent information, and the programming of appropriate machines to carry out a search.},
	language = {en},
	number = {4},
	urldate = {2021-12-06},
	journal = {IBM Journal of Research and Development},
	author = {Luhn, H. P.},
	month = oct,
	year = {1957},
	pages = {309--317},
	file = {Luhn - 1957 - A Statistical Approach to Mechanized Encoding and .pdf:/Users/kmfrick/Documents/Zotero/storage/QTRWTFVW/Luhn - 1957 - A Statistical Approach to Mechanized Encoding and .pdf:application/pdf},
}

@article{bojanowski_enriching_2016,
	title = {Enriching {Word} {Vectors} with {Subword} {Information}},
	url = {https://arxiv.org/abs/1607.04606v2},
	abstract = {Continuous word representations, trained on large unlabeled corpora are useful for many natural language processing tasks. Popular models that learn such representations ignore the morphology of words, by assigning a distinct vector to each word. This is a limitation, especially for languages with large vocabularies and many rare words. In this paper, we propose a new approach based on the skipgram model, where each word is represented as a bag of character \$n\$-grams. A vector representation is associated to each character \$n\$-gram; words being represented as the sum of these representations. Our method is fast, allowing to train models on large corpora quickly and allows us to compute word representations for words that did not appear in the training data. We evaluate our word representations on nine different languages, both on word similarity and analogy tasks. By comparing to recently proposed morphological word representations, we show that our vectors achieve state-of-the-art performance on these tasks.},
	language = {en},
	urldate = {2021-12-02},
	author = {Bojanowski, Piotr and Grave, Edouard and Joulin, Armand and Mikolov, Tomas},
	month = jul,
	year = {2016},
	file = {Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/NX9I2A6X/Bojanowski et al. - 2016 - Enriching Word Vectors with Subword Information.pdf:application/pdf;Snapshot:/Users/kmfrick/Documents/Zotero/storage/DPNAQSED/1607.html:text/html},
}

@inproceedings{lhoest_datasets_2021,
	address = {Online and Punta Cana, Dominican Republic},
	title = {Datasets: {A} {Community} {Library} for {Natural} {Language} {Processing}},
	url = {https://aclanthology.org/2021.emnlp-demo.21},
	abstract = {The scale, variety, and quantity of publicly-available NLP datasets has grown rapidly as researchers propose new tasks, larger models, and novel benchmarks. Datasets is a community library for contemporary NLP designed to support this ecosystem. Datasets aims to standardize end-user interfaces, versioning, and documentation, while providing a lightweight front-end that behaves similarly for small datasets as for internet-scale corpora. The design of the library incorporates a distributed, community-driven approach to adding datasets and documenting usage. After a year of development, the library now includes more than 650 unique datasets, has more than 250 contributors, and has helped support a variety of novel cross-dataset research projects and shared tasks. The library is available at https://github.com/huggingface/datasets.},
	booktitle = {Proceedings of the 2021 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Lhoest, Quentin and Villanova del Moral, Albert and Jernite, Yacine and Thakur, Abhishek and von Platen, Patrick and Patil, Suraj and Chaumond, Julien and Drame, Mariama and Plu, Julien and Tunstall, Lewis and Davison, Joe and Šaško, Mario and Chhablani, Gunjan and Malik, Bhavitvya and Brandeis, Simon and Le Scao, Teven and Sanh, Victor and Xu, Canwen and Patry, Nicolas and McMillan-Major, Angelina and Schmid, Philipp and Gugger, Sylvain and Delangue, Clément and Matussière, Théo and Debut, Lysandre and Bekman, Stas and Cistac, Pierric and Goehringer, Thibault and Mustar, Victor and Lagunas, François and Rush, Alexander and Wolf, Thomas},
	month = nov,
	year = {2021},
	note = {\_eprint: 2109.02846},
	pages = {175--184},
}

@inproceedings{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	volume = {26},
	url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
	urldate = {2021-12-02},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	year = {2013},
	file = {Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/GWXF8VG9/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf},
}

@misc{lhoest_huggingfacedatasets_2021,
	title = {huggingface/datasets: 1.15.1},
	url = {https://doi.org/10.5281/zenodo.5639822},
	publisher = {Zenodo},
	author = {Lhoest, Quentin and Moral, Albert Villanova del and Platen, Patrick von and Wolf, Thomas and Šaško, Mario and Jernite, Yacine and Thakur, Abhishek and Tunstall, Lewis and Patil, Suraj and Drame, Mariama and Chaumond, Julien and Plu, Julien and Davison, Joe and Brandeis, Simon and Sanh, Victor and Scao, Teven Le and Xu, Kevin Canwen and Patry, Nicolas and Liu, Steven and McMillan-Major, Angelina and Schmid, Philipp and Gugger, Sylvain and Raw, Nathan and Lesage, Sylvain and Lozhkov, Anton and Carrigan, Matthew and Matussière, Théo and Werra, Leandro von and Debut, Lysandre and Bekman, Stas and Delangue, Clément},
	month = nov,
	year = {2021},
	doi = {10.5281/zenodo.5639822},
}

@book{hastie_elements_2013,
	title = {The {Elements} of {Statistical} {Learning}: {Data} {Mining}, {Inference}, and {Prediction}},
	isbn = {978-0-387-21606-5},
	shorttitle = {The {Elements} of {Statistical} {Learning}},
	abstract = {During the past decade there has been an explosion in computation and information technology. With it have come vast amounts of data in a variety of fields such as medicine, biology, finance, and marketing. The challenge of understanding these data has led to the development of new tools in the field of statistics, and spawned new areas such as data mining, machine learning, and bioinformatics. Many of these tools have common underpinnings but are often expressed with different terminology. This book describes the important ideas in these areas in a common conceptual framework. While the approach is statistical, the emphasis is on concepts rather than mathematics. Many examples are given, with a liberal use of color graphics. It is a valuable resource for statisticians and anyone interested in data mining in science or industry. The book's coverage is broad, from supervised learning (prediction) to unsupervised learning. The many topics include neural networks, support vector machines, classification trees and boosting---the first comprehensive treatment of this topic in any book. This major new edition features many topics not covered in the original, including graphical models, random forests, ensemble methods, least angle regression \& path algorithms for the lasso, non-negative matrix factorization, and spectral clustering. There is also a chapter on methods for ``wide'' data (p bigger than n), including multiple testing and false discovery rates. Trevor Hastie, Robert Tibshirani, and Jerome Friedman are professors of statistics at Stanford University. They are prominent researchers in this area: Hastie and Tibshirani developed generalized additive models and wrote a popular book of that title. Hastie co-developed much of the statistical modeling software and environment in R/S-PLUS and invented principal curves and surfaces. Tibshirani proposed the lasso and is co-author of the very successful An Introduction to the Bootstrap. Friedman is the co-inventor of many data-mining tools including CART, MARS, projection pursuit and gradient boosting.},
	language = {en},
	publisher = {Springer Science \& Business Media},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome},
	month = nov,
	year = {2013},
	note = {Google-Books-ID: yPfZBwAAQBAJ},
	keywords = {Computers / Artificial Intelligence / General, Computers / Database Administration \& Management, Computers / Mathematical \& Statistical Software, Mathematics / Discrete Mathematics, Mathematics / Probability \& Statistics / General, Mathematics / Probability \& Statistics / Stochastic Processes, Science / Life Sciences / Biology, Science / Life Sciences / General},
}

@inproceedings{gupta_improving_2019,
	address = {Florence, Italy},
	title = {Improving {Word} {Embeddings} {Using} {Kernel} {PCA}},
	url = {https://aclanthology.org/W19-4323},
	doi = {10.18653/v1/W19-4323},
	abstract = {Word-based embedding approaches such as Word2Vec capture the meaning of words and relations between them, particularly well when trained with large text collections; however, they fail to do so with small datasets. Extensions such as fastText reduce the amount of data needed slightly, however, the joint task of learning meaningful morphology, syntactic and semantic representations still requires a lot of data. In this paper, we introduce a new approach to warm-start embedding models with morphological information, in order to reduce training time and enhance their performance. We use word embeddings generated using both word2vec and fastText models and enrich them with morphological information of words, derived from kernel principal component analysis (KPCA) of word similarity matrices. This can be seen as explicitly feeding the network morphological similarities and letting it learn semantic and syntactic similarities. Evaluating our models on word similarity and analogy tasks in English and German, we find that they not only achieve higher accuracies than the original skip-gram and fastText models but also require significantly less training data and time. Another benefit of our approach is that it is capable of generating a high-quality representation of infrequent words as, for example, found in very recent news articles with rapidly changing vocabularies. Lastly, we evaluate the different models on a downstream sentence classification task in which a CNN model is initialized with our embeddings and find promising results.},
	urldate = {2021-11-28},
	booktitle = {Proceedings of the 4th {Workshop} on {Representation} {Learning} for {NLP} ({RepL4NLP}-2019)},
	publisher = {Association for Computational Linguistics},
	author = {Gupta, Vishwani and Giesselbach, Sven and Rüping, Stefan and Bauckhage, Christian},
	month = aug,
	year = {2019},
	pages = {200--208},
	file = {Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/IT5XKK8E/Gupta et al. - 2019 - Improving Word Embeddings Using Kernel PCA.pdf:application/pdf},
}

@incollection{karlgren_words_2001,
	title = {From words to understanding},
	booktitle = {Computing with {Large} {Random} {Patterns}},
	author = {Karlgren, Jussi and Sahlgren, Magnus},
	year = {2001},
	pages = {294--308},
	file = {FULLTEXT01.pdf:/Users/kmfrick/Documents/Zotero/storage/CDTB8YSE/FULLTEXT01.pdf:application/pdf},
}

@article{arora_latent_2016,
	title = {A {Latent} {Variable} {Model} {Approach} to {PMI}-based {Word} {Embeddings}},
	volume = {4},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00106},
	doi = {10.1162/tacl_a_00106},
	abstract = {Semantic word embeddings represent the meaning of a word via a vector, and are
created by diverse methods. Many use nonlinear operations on co-occurrence
statistics, and have hand-tuned hyperparameters and reweighting methods.This paper proposes a new generative model, a dynamic version of the log-linear
topic model of Mnih and Hinton (2007). The methodological novelty is to use the
prior to compute closed form expressions for word statistics. This provides a
theoretical justification for nonlinear models like PMI, word2vec, and GloVe, as
well as some hyperparameter choices. It also helps explain why low-dimensional
semantic embeddings contain linear algebraic structure that allows solution of
word analogies, as shown by Mikolov et al. (2013a) and many subsequent
papers.Experimental support is provided for the generative model assumptions, the most
important of which is that latent word vectors are fairly uniformly dispersed in
space.},
	urldate = {2021-12-06},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Arora, Sanjeev and Li, Yuanzhi and Liang, Yingyu and Ma, Tengyu and Risteski, Andrej},
	month = jul,
	year = {2016},
	pages = {385--399},
	file = {Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/3VIE8MRD/Arora et al. - 2016 - A Latent Variable Model Approach to PMI-based Word.pdf:application/pdf;Snapshot:/Users/kmfrick/Documents/Zotero/storage/76UNJ7NC/A-Latent-Variable-Model-Approach-to-PMI-based-Word.html:text/html},
}

@article{fermanian_framing_2021,
	title = {Framing {RNN} as a kernel method: {A} neural {ODE} approach},
	shorttitle = {Framing {RNN} as a kernel method},
	url = {https://arxiv.org/abs/2106.01202v2},
	abstract = {Building on the interpretation of a recurrent neural network (RNN) as a continuous-time neural differential equation, we show, under appropriate conditions, that the solution of a RNN can be viewed as a linear function of a specific feature set of the input sequence, known as the signature. This connection allows us to frame a RNN as a kernel method in a suitable reproducing kernel Hilbert space. As a consequence, we obtain theoretical guarantees on generalization and stability for a large class of recurrent networks. Our results are illustrated on simulated datasets.},
	language = {en},
	urldate = {2021-12-07},
	author = {Fermanian, Adeline and Marion, Pierre and Vert, Jean-Philippe and Biau, Gérard},
	month = jun,
	year = {2021},
	file = {Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/LLZJCKXV/Fermanian et al. - 2021 - Framing RNN as a kernel method A neural ODE appro.pdf:application/pdf;Snapshot:/Users/kmfrick/Documents/Zotero/storage/UT4MFINS/2106.html:text/html},
}

@article{yepes_word_2017,
	title = {Word embeddings and recurrent neural networks based on {Long}-{Short} {Term} {Memory} nodes in supervised biomedical word sense disambiguation},
	volume = {73},
	issn = {1532-0464},
	url = {https://www.sciencedirect.com/science/article/pii/S1532046417301806},
	doi = {https://doi.org/10.1016/j.jbi.2017.08.001},
	abstract = {Word sense disambiguation helps identifying the proper sense of ambiguous words in text. With large terminologies such as the UMLS Metathesaurus ambiguities appear and highly effective disambiguation methods are required. Supervised learning algorithm methods are used as one of the approaches to perform disambiguation. Features extracted from the context of an ambiguous word are used to identify the proper sense of such a word. The type of features have an impact on machine learning methods, thus affect disambiguation performance. In this work, we have evaluated several types of features derived from the context of the ambiguous word and we have explored as well more global features derived from MEDLINE using word embeddings. Results show that word embeddings improve the performance of more traditional features and allow as well using recurrent neural network classifiers based on Long-Short Term Memory (LSTM) nodes. The combination of unigrams and word embeddings with an SVM sets a new state of the art performance with a macro accuracy of 95.97 in the MSH WSD data set.},
	journal = {Journal of Biomedical Informatics},
	author = {Yepes, Antonio Jimeno},
	year = {2017},
	keywords = {Biomedical domain, Recurrent neural networks, Word embeddings, Word sense disambiguation},
	pages = {137--147},
}

@misc{noauthor_senteval_2021,
	title = {{SentEval}: evaluation toolkit for sentence embeddings},
	shorttitle = {{SentEval}},
	url = {https://github.com/facebookresearch/SentEval},
	abstract = {A python tool for evaluating the quality of sentence embeddings.},
	urldate = {2021-12-07},
	publisher = {Meta Research},
	month = dec,
	year = {2021},
	note = {original-date: 2017-05-18T20:44:47Z},
}

@article{conneau_senteval_2018,
	title = {{SentEval}: {An} {Evaluation} {Toolkit} for {Universal} {Sentence} {Representations}},
	shorttitle = {{SentEval}},
	url = {http://arxiv.org/abs/1803.05449},
	abstract = {We introduce SentEval, a toolkit for evaluating the quality of universal sentence representations. SentEval encompasses a variety of tasks, including binary and multi-class classification, natural language inference and sentence similarity. The set of tasks was selected based on what appears to be the community consensus regarding the appropriate evaluations for universal sentence representations. The toolkit comes with scripts to download and preprocess datasets, and an easy interface to evaluate sentence encoders. The aim is to provide a fairer, less cumbersome and more centralized way for evaluating sentence representations.},
	urldate = {2021-12-07},
	journal = {arXiv:1803.05449 [cs]},
	author = {Conneau, Alexis and Kiela, Douwe},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.05449},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: LREC 2018},
	file = {arXiv.org Snapshot:/Users/kmfrick/Documents/Zotero/storage/LU35DYBI/1803.html:text/html;Conneau_Kiela_2018_SentEval.pdf:/Users/kmfrick/Documents/Zotero/storage/DP6PQGCT/Conneau_Kiela_2018_SentEval.pdf:application/pdf},
}

@inproceedings{jurgens_semeval-2012_2012,
	address = {Montréal, Canada},
	title = {{SemEval}-2012 {Task} 2: {Measuring} {Degrees} of {Relational} {Similarity}},
	shorttitle = {{SemEval}-2012 {Task} 2},
	url = {https://aclanthology.org/S12-1047},
	urldate = {2021-12-07},
	booktitle = {*{SEM} 2012: {The} {First} {Joint} {Conference} on {Lexical} and {Computational} {Semantics} – {Volume} 1: {Proceedings} of the main conference and the shared task, and {Volume} 2: {Proceedings} of the {Sixth} {International} {Workshop} on {Semantic} {Evaluation} ({SemEval} 2012)},
	publisher = {Association for Computational Linguistics},
	author = {Jurgens, David and Mohammad, Saif and Turney, Peter and Holyoak, Keith},
	month = jul,
	year = {2012},
	pages = {356--364},
	file = {Jurgens et al_2012_SemEval-2012 Task 2.pdf:/Users/kmfrick/Documents/Zotero/storage/FGMBLHJ9/Jurgens et al_2012_SemEval-2012 Task 2.pdf:application/pdf},
}

@inproceedings{mikolov_linguistic_2013,
	address = {Atlanta, Georgia},
	title = {Linguistic {Regularities} in {Continuous} {Space} {Word} {Representations}},
	url = {https://aclanthology.org/N13-1090},
	urldate = {2021-12-06},
	booktitle = {Proceedings of the 2013 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
	month = jun,
	year = {2013},
	pages = {746--751},
	file = {Mikolov et al_2013_Linguistic Regularities in Continuous Space Word Representations.pdf:/Users/kmfrick/Documents/Zotero/storage/ZD3C7B2D/Mikolov et al_2013_Linguistic Regularities in Continuous Space Word Representations.pdf:application/pdf},
}

@inproceedings{matsuoka_measuring_2014,
	title = {Measuring {Similarity} from {Word} {Pair} {Matrices} with {Syntagmatic} and {Paradigmatic} {Associations}},
	url = {10.3115/v1/W14-4712},
	doi = {10.3115/v1/W14-4712},
	abstract = {Two types of semantic similarity are usually distinguished: attributional and relational similarities. These similarities measure the degree between words or word pairs. Attributional similarities are bidrectional, while relational similarities are one-directional. It is possible to compute such similarities based on the occurrences of words in actual sentences. Inside sentences, syntagmatic associations and paradigmatic associations can be used to characterize the relations between words or word pairs. In this paper, we propose a vector space model built from syntagmatic and paradigmatic associations to measure relational similarity between word pairs from the sentences contained in a small corpus. We conduct two experiments with different datasets: SemEval-2012 task 2, and 400 word analogy quizzes. The experimental results show that our proposed method is effective when using a small corpus.},
	author = {Matsuoka, Jin and Lepage, Yves},
	month = jan,
	year = {2014},
	pages = {77--86},
	file = {Matsuoka_Lepage_2014_Measuring Similarity from Word Pair Matrices with Syntagmatic and Paradigmatic.pdf:/Users/kmfrick/Documents/Zotero/storage/4EJTUXIQ/Matsuoka_Lepage_2014_Measuring Similarity from Word Pair Matrices with Syntagmatic and Paradigmatic.pdf:application/pdf},
}

@article{mikolov_efficient_2013,
	title = {Efficient {Estimation} of {Word} {Representations} in {Vector} {Space}},
	url = {http://arxiv.org/abs/1301.3781},
	abstract = {We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities.},
	urldate = {2021-12-06},
	journal = {arXiv:1301.3781 [cs]},
	author = {Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	month = sep,
	year = {2013},
	note = {arXiv: 1301.3781},
	keywords = {Computer Science - Computation and Language},
	file = {arXiv.org Snapshot:/Users/kmfrick/Documents/Zotero/storage/FX5GFY47/1301.html:text/html;Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf:/Users/kmfrick/Documents/Zotero/storage/SQLWANWX/Mikolov et al_2013_Efficient Estimation of Word Representations in Vector Space.pdf:application/pdf},
}

@article{raunak_simple_2017,
	title = {Simple and {Effective} {Dimensionality} {Reduction} for {Word} {Embeddings}},
	url = {http://arxiv.org/abs/1708.03629},
	abstract = {Word embeddings have become the basic building blocks for several natural language processing and information retrieval tasks. Pre-trained word embeddings are used in several downstream applications as well as for constructing representations for sentences, paragraphs and documents. Recently, there has been an emphasis on further improving the pre-trained word vectors through post-processing algorithms. One such area of improvement is the dimensionality reduction of the word embeddings. Reducing the size of word embeddings through dimensionality reduction can improve their utility in memory constrained devices, beneﬁting several real-world applications. In this work, we present a novel algorithm that effectively combines PCA based dimensionality reduction with a recently proposed post-processing algorithm, to construct word embeddings of lower dimensions. Empirical evaluations on 12 standard word similarity benchmarks show that our algorithm reduces the embedding dimensionality by 50\%, while achieving similar or (more often) better performance than the higher dimension embeddings.},
	language = {en},
	urldate = {2021-12-08},
	journal = {arXiv:1708.03629 [cs]},
	author = {Raunak, Vikas},
	month = nov,
	year = {2017},
	note = {arXiv: 1708.03629},
	keywords = {Computer Science - Computation and Language},
	annote = {Comment: Accepted at NIPS 2017 LLD Workshop},
	file = {Raunak - 2017 - Simple and Effective Dimensionality Reduction for .pdf:/Users/kmfrick/Documents/Zotero/storage/P43ZF7IX/Raunak - 2017 - Simple and Effective Dimensionality Reduction for .pdf:application/pdf},
}

@inproceedings{bhatia_morphological_2016,
	address = {Austin, Texas},
	title = {Morphological {Priors} for {Probabilistic} {Neural} {Word} {Embeddings}},
	url = {https://aclanthology.org/D16-1047},
	doi = {10.18653/v1/D16-1047},
	urldate = {2021-12-08},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Bhatia, Parminder and Guthrie, Robert and Eisenstein, Jacob},
	month = nov,
	year = {2016},
	pages = {490--500},
	file = {Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/7TZ8T2NA/Bhatia et al. - 2016 - Morphological Priors for Probabilistic Neural Word.pdf:application/pdf},
}

@article{mu_all-but--top_2017,
	title = {All-but-the-{Top}: {Simple} and {Effective} {Postprocessing} for {Word} {Representations}},
	shorttitle = {All-but-the-{Top}},
	url = {https://arxiv.org/abs/1702.01417v2},
	abstract = {Real-valued word representations have transformed NLP applications; popular examples are word2vec and GloVe, recognized for their ability to capture linguistic regularities. In this paper, we demonstrate a \{{\textbackslash}em very simple\}, and yet counter-intuitive, postprocessing technique -- eliminate the common mean vector and a few top dominating directions from the word vectors -- that renders off-the-shelf representations \{{\textbackslash}em even stronger\}. The postprocessing is empirically validated on a variety of lexical-level intrinsic tasks (word similarity, concept categorization, word analogy) and sentence-level tasks (semantic textural similarity and \{ text classification\}) on multiple datasets and with a variety of representation methods and hyperparameter choices in multiple languages; in each case, the processed representations are consistently better than the original ones.},
	language = {en},
	urldate = {2021-12-08},
	author = {Mu, Jiaqi and Bhat, Suma and Viswanath, Pramod},
	month = feb,
	year = {2017},
	file = {Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/T5FTTJH5/Mu et al. - 2017 - All-but-the-Top Simple and Effective Postprocessi.pdf:application/pdf;Snapshot:/Users/kmfrick/Documents/Zotero/storage/IDC9DYEA/1702.html:text/html},
}

@article{loshchilov_sgdr_2016,
	title = {{SGDR}: {Stochastic} {Gradient} {Descent} with {Warm} {Restarts}},
	shorttitle = {{SGDR}},
	url = {https://arxiv.org/abs/1608.03983v5},
	abstract = {Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14\% and 16.21\%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR},
	language = {en},
	urldate = {2021-12-08},
	author = {Loshchilov, Ilya and Hutter, Frank},
	month = aug,
	year = {2016},
	file = {Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/C5APKKSN/Loshchilov and Hutter - 2016 - SGDR Stochastic Gradient Descent with Warm Restar.pdf:application/pdf;Snapshot:/Users/kmfrick/Documents/Zotero/storage/MB8BGS49/1608.html:text/html},
}

@misc{alam_hyperparameter_2014,
	title = {Hyperparameter {Selection} in {Kernel} {Principal} {Component} {Analysis}},
	abstract = {In kernel methods, choosing a suitable kernel is indispensable for favorable results. No well-founded methods, however, have been established in general for unsupervised learning. We focus on kernel Principal Component Analysis (kernel PCA), which is a nonlinear extension of principal component analysis and has been used electively for extracting nonlinear features and reducing dimensionality. As a kernel method, kernel PCA also suffers from the problem of kernel choice. Although cross-validation is a popular method for choosing hyperparameters, it is not applicable straightforwardly to choose a kernel in kernel PCA because of the incomparable norms given by different kernels. It is important, thus, to develop a well-founded method for choosing a kernel in kernel PCA. This study proposes a method for choosing hyperparameters in kernel PCA (kernel and the number of components) based on cross-validation for the comparable reconstruction errors of pre-images in the original space. The experimental results on synthesized and real-world datasets demonstrate that the proposed method successfully selects an appropriate kernel and the number of components in kernel PCA in terms of visualization and classification errors on the principal components. The results imply that the proposed method enables automatic design of hyperparameters in kernel PCA.},
	author = {Alam, Md Ashad and Fukumizu, Kenji},
	year = {2014},
	file = {Citeseer - Full Text PDF:/Users/kmfrick/Documents/Zotero/storage/C2R4LCGW/Alam and Fukumizu - Hyperparameter Selection in Kernel Principal Compo.pdf:application/pdf;Citeseer - Snapshot:/Users/kmfrick/Documents/Zotero/storage/ZX2C3PKG/summary.html:text/html},
}

@inproceedings{mika_fisher_1999,
	title = {Fisher discriminant analysis with kernels},
	doi = {10.1109/NNSP.1999.788121},
	abstract = {A non-linear classification technique based on Fisher's discriminant is proposed. The main ingredient is the kernel trick which allows the efficient computation of Fisher discriminant in feature space. The linear classification in feature space corresponds to a (powerful) non-linear decision function in input space. Large scale simulations demonstrate the competitiveness of our approach.},
	booktitle = {Neural {Networks} for {Signal} {Processing} {IX}: {Proceedings} of the 1999 {IEEE} {Signal} {Processing} {Society} {Workshop} ({Cat}. {No}.{98TH8468})},
	author = {Mika, S. and Ratsch, G. and Weston, J. and Scholkopf, B. and Mullers, K.R.},
	month = aug,
	year = {1999},
	keywords = {Algorithm design and analysis, Closed-form solution, Computational modeling, Feature extraction, Gaussian distribution, Kernel, Large-scale systems, Principal component analysis, Probability, Support vector machines},
	pages = {41--48},
	file = {IEEE Xplore Abstract Record:/Users/kmfrick/Documents/Zotero/storage/32LYX7SM/788121.html:text/html},
}

@inproceedings{wolf-etal-2020-transformers,
    title = "Transformers: State-of-the-Art Natural Language Processing",
    author = "Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and Rémi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush",
    booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations",
    month = oct,
    year = "2020",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/2020.emnlp-demos.6",
    pages = "38--45"
}